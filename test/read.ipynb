{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try reading gnomAD data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Revise\n",
    "using gnomAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-02 11:01:03.155 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.1.3\n",
      "SparkUI available at http://dn52ecmi.sunet:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.109-b71b065e4bb6\n",
      "LOGGING: writing to /Users/biona001/.julia/dev/gnomAD/test/hail-20230302-1101-0.2.109-b71b065e4bb6.log\n",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "PyCall.PyError",
     "evalue": "PyError ($(Expr(:escape, :(ccall(#= /Users/biona001/.julia/packages/PyCall/twYvK/src/PyCall.jl:783 =# @pysym(:PyObject_GetItem), PyPtr, (PyPtr, PyPtr), o, PyObject(k)))))) <class 'hail.utils.java.FatalError'>\nFatalError('FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist\\n\\nJava stack trace:\\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 56) (dn52ecmi.sunet executor driver): java.io.FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\\n\\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\\n\\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\\n\\tat is.hail.io.fs.HadoopFS.openNoCompression(HadoopFS.scala:92)\\n\\tat is.hail.io.fs.FS.open(FS.scala:360)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:359)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:372)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:371)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:369)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:368)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.HailContext$$anon$1.compute(HailContext.scala:173)\\n\\tat is.hail.utils.richUtils.RichRDD$$anon$1.compute(RichRDD.scala:152)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n\\nDriver stacktrace:\\n\\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2303)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2252)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2251)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2251)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\\n\\tat scala.Option.foreach(Option.scala:407)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2490)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\\n\\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\\n\\tat is.hail.linalg.BlockMatrix.getElement(BlockMatrix.scala:1305)\\n\\tat is.hail.expr.ir.functions.GetElement.execute(GetElement.scala:13)\\n\\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:872)\\n\\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:58)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72)\\n\\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16)\\n\\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16)\\n\\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13)\\n\\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64)\\n\\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22)\\n\\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20)\\n\\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\\n\\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\\n\\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\\n\\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20)\\n\\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47)\\n\\tat is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:453)\\n\\tat is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:489)\\n\\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:73)\\n\\tat is.hail.utils.package$.using(package.scala:635)\\n\\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:73)\\n\\tat is.hail.utils.package$.using(package.scala:635)\\n\\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17)\\n\\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:61)\\n\\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:341)\\n\\tat is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:486)\\n\\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\\n\\tat is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:485)\\n\\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n\\njava.io.FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\\n\\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\\n\\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\\n\\tat is.hail.io.fs.HadoopFS.openNoCompression(HadoopFS.scala:92)\\n\\tat is.hail.io.fs.FS.open(FS.scala:360)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:359)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:372)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:371)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:369)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:368)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.HailContext$$anon$1.compute(HailContext.scala:173)\\n\\tat is.hail.utils.richUtils.RichRDD$$anon$1.compute(RichRDD.scala:152)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n\\n\\n\\n\\nHail version: 0.2.109-b71b065e4bb6\\nError summary: FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist')\n  File \"<decorator-gen-1466>\", line 2, in __getitem__\n  File \"/usr/local/lib/python3.9/site-packages/hail/typecheck/check.py\", line 577, in wrapper\n    return __original_func(*args_, **kwargs_)\n  File \"/usr/local/lib/python3.9/site-packages/hail/linalg/blockmatrix.py\", line 898, in __getitem__\n    return Env.backend().execute(BlockMatrixToValueApply(self._bmir,\n  File \"/usr/local/lib/python3.9/site-packages/hail/backend/py4j_backend.py\", line 105, in execute\n    raise e.maybe_user_error(ir) from None\n  File \"/usr/local/lib/python3.9/site-packages/hail/backend/py4j_backend.py\", line 99, in execute\n    result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed)\n  File \"/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/lib/python3.9/site-packages/hail/backend/py4j_backend.py\", line 31, in deco\n    raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None\n",
     "output_type": "error",
     "traceback": [
      "PyError ($(Expr(:escape, :(ccall(#= /Users/biona001/.julia/packages/PyCall/twYvK/src/PyCall.jl:783 =# @pysym(:PyObject_GetItem), PyPtr, (PyPtr, PyPtr), o, PyObject(k)))))) <class 'hail.utils.java.FatalError'>\nFatalError('FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist\\n\\nJava stack trace:\\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 56) (dn52ecmi.sunet executor driver): java.io.FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\\n\\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\\n\\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\\n\\tat is.hail.io.fs.HadoopFS.openNoCompression(HadoopFS.scala:92)\\n\\tat is.hail.io.fs.FS.open(FS.scala:360)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:359)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:372)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:371)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:369)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:368)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.HailContext$$anon$1.compute(HailContext.scala:173)\\n\\tat is.hail.utils.richUtils.RichRDD$$anon$1.compute(RichRDD.scala:152)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n\\nDriver stacktrace:\\n\\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2303)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2252)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2251)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2251)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\\n\\tat scala.Option.foreach(Option.scala:407)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2490)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\\n\\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\\n\\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\\n\\tat is.hail.linalg.BlockMatrix.getElement(BlockMatrix.scala:1305)\\n\\tat is.hail.expr.ir.functions.GetElement.execute(GetElement.scala:13)\\n\\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:872)\\n\\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:58)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53)\\n\\tat is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72)\\n\\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16)\\n\\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16)\\n\\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14)\\n\\tat is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13)\\n\\tat is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64)\\n\\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22)\\n\\tat is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20)\\n\\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\\n\\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\\n\\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\\n\\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20)\\n\\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47)\\n\\tat is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:453)\\n\\tat is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:489)\\n\\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:73)\\n\\tat is.hail.utils.package$.using(package.scala:635)\\n\\tat is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:73)\\n\\tat is.hail.utils.package$.using(package.scala:635)\\n\\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17)\\n\\tat is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:61)\\n\\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:341)\\n\\tat is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:486)\\n\\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\\n\\tat is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:485)\\n\\tat sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n\\njava.io.FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\\n\\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\\n\\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\\n\\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\\n\\tat is.hail.io.fs.HadoopFS.openNoCompression(HadoopFS.scala:92)\\n\\tat is.hail.io.fs.FS.open(FS.scala:360)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:359)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:372)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:371)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.io.fs.FS.open(FS.scala:369)\\n\\tat is.hail.io.fs.FS.open$(FS.scala:368)\\n\\tat is.hail.io.fs.HadoopFS.open(HadoopFS.scala:72)\\n\\tat is.hail.HailContext$$anon$1.compute(HailContext.scala:173)\\n\\tat is.hail.utils.richUtils.RichRDD$$anon$1.compute(RichRDD.scala:152)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n\\tat java.lang.Thread.run(Thread.java:745)\\n\\n\\n\\n\\nHail version: 0.2.109-b71b065e4bb6\\nError summary: FileNotFoundException: File /Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm/parts/part-10022-14-10022-0-216231b4-e554-eac9-d165-ffa8a90fef05 does not exist')\n  File \"<decorator-gen-1466>\", line 2, in __getitem__\n  File \"/usr/local/lib/python3.9/site-packages/hail/typecheck/check.py\", line 577, in wrapper\n    return __original_func(*args_, **kwargs_)\n  File \"/usr/local/lib/python3.9/site-packages/hail/linalg/blockmatrix.py\", line 898, in __getitem__\n    return Env.backend().execute(BlockMatrixToValueApply(self._bmir,\n  File \"/usr/local/lib/python3.9/site-packages/hail/backend/py4j_backend.py\", line 105, in execute\n    raise e.maybe_user_error(ir) from None\n  File \"/usr/local/lib/python3.9/site-packages/hail/backend/py4j_backend.py\", line 99, in execute\n    result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed)\n  File \"/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/lib/python3.9/site-packages/hail/backend/py4j_backend.py\", line 31, in deco\n    raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None\n",
      "",
      "Stacktrace:",
      "  [1] pyerr_check",
      "    @ ~/.julia/packages/PyCall/twYvK/src/exception.jl:75 [inlined]",
      "  [2] pyerr_check",
      "    @ ~/.julia/packages/PyCall/twYvK/src/exception.jl:79 [inlined]",
      "  [3] _handle_error(msg::String)",
      "    @ PyCall ~/.julia/packages/PyCall/twYvK/src/exception.jl:96",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/PyCall/twYvK/src/exception.jl:110 [inlined]",
      "  [5] get(o::PyCall.PyObject, returntype::Type{PyCall.PyAny}, k::Tuple{Int64, Int64})",
      "    @ PyCall ~/.julia/packages/PyCall/twYvK/src/PyCall.jl:782",
      "  [6] get",
      "    @ ~/.julia/packages/PyCall/twYvK/src/PyCall.jl:787 [inlined]",
      "  [7] getindex(o::PyCall.PyObject, i1::Int64, i2::Int64)",
      "    @ PyCall ./deprecated.jl:72",
      "  [8] getindex",
      "    @ ~/.julia/dev/gnomAD/src/hailBM.jl:27 [inlined]",
      "  [9] isassigned(::gnomAD.HailBlockMatrix, ::Int64, ::Int64)",
      "    @ Base ./abstractarray.jl:553",
      " [10] alignment(io::IOContext{IOBuffer}, X::AbstractVecOrMat, rows::Vector{Int64}, cols::Vector{Int64}, cols_if_complete::Int64, cols_otherwise::Int64, sep::Int64)",
      "    @ Base ./arrayshow.jl:67",
      " [11] _print_matrix(io::IOContext{IOBuffer}, X::AbstractVecOrMat, pre::String, sep::String, post::String, hdots::String, vdots::String, ddots::String, hmod::Int64, vmod::Int64, rowsA::UnitRange{Int64}, colsA::UnitRange{Int64})",
      "    @ Base ./arrayshow.jl:243",
      " [12] print_matrix(io::IOContext{IOBuffer}, X::gnomAD.HailBlockMatrix, pre::String, sep::String, post::String, hdots::String, vdots::String, ddots::String, hmod::Int64, vmod::Int64) (repeats 2 times)",
      "    @ Base ./arrayshow.jl:169",
      " [13] print_array",
      "    @ ./arrayshow.jl:355 [inlined]",
      " [14] show(io::IOContext{IOBuffer}, #unused#::MIME{Symbol(\"text/plain\")}, X::gnomAD.HailBlockMatrix)",
      "    @ Base ./arrayshow.jl:396",
      " [15] limitstringmime(mime::MIME{Symbol(\"text/plain\")}, x::gnomAD.HailBlockMatrix)",
      "    @ IJulia ~/.julia/packages/IJulia/AQu2H/src/inline.jl:43",
      " [16] display_mimestring",
      "    @ ~/.julia/packages/IJulia/AQu2H/src/display.jl:71 [inlined]",
      " [17] display_dict(x::gnomAD.HailBlockMatrix)",
      "    @ IJulia ~/.julia/packages/IJulia/AQu2H/src/display.jl:102",
      " [18] #invokelatest#2",
      "    @ ./essentials.jl:716 [inlined]",
      " [19] invokelatest",
      "    @ ./essentials.jl:714 [inlined]",
      " [20] execute_request(socket::ZMQ.Socket, msg::IJulia.Msg)",
      "    @ IJulia ~/.julia/packages/IJulia/AQu2H/src/execute_request.jl:112",
      " [21] #invokelatest#2",
      "    @ ./essentials.jl:716 [inlined]",
      " [22] invokelatest",
      "    @ ./essentials.jl:714 [inlined]",
      " [23] eventloop(socket::ZMQ.Socket)",
      "    @ IJulia ~/.julia/packages/IJulia/AQu2H/src/eventloop.jl:8",
      " [24] (::IJulia.var\"#15#18\")()",
      "    @ IJulia ./task.jl:429"
     ]
    }
   ],
   "source": [
    "data = \"/Users/biona001/.julia/dev/gnomAD/data/gnomad.genomes.r2.1.1.nfe.common.adj.ld.bm\"\n",
    "bm = hail_block_matrix(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-01 16:39:40.545 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      " 21.392457 seconds (11.79 k allocations: 673.166 KiB, 0.09% compilation time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.1.3\n",
      "SparkUI available at http://dn52ecqn.sunet:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.109-b71b065e4bb6\n",
      "LOGGING: writing to /Users/biona001/.julia/dev/gnomAD/test/hail-20230301-1639-0.2.109-b71b065e4bb6.log\n",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×10 Matrix{Float64}:\n",
       " 1.0  0.0326361  -0.0181891   0.0166341   …   0.0179389   -0.00994853\n",
       " 0.0  1.0        -0.00041394  0.00540477      0.00211069   0.00171555\n",
       " 0.0  0.0         1.0         0.0177243       0.0         -0.000284646\n",
       " 0.0  0.0         0.0         1.0             0.00627614   0.00131174\n",
       " 0.0  0.0         0.0         0.0            -0.0045831    0.00119736\n",
       " 0.0  0.0         0.0         0.0         …   0.0145138    0.00530853\n",
       " 0.0  0.0         0.0         0.0             0.00089687   0.00328037\n",
       " 0.0  0.0         0.0         0.0             0.123663     0.377178\n",
       " 0.0  0.0         0.0         0.0             1.0          0.00725709\n",
       " 0.0  0.0         0.0         0.0             0.0          1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time begin\n",
    "    out = zeros(10, 10)\n",
    "    for i in 1:10, j in 1:10\n",
    "        out[i, j] = bm[i, j]\n",
    "    end\n",
    "end\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.471002 seconds (67 allocations: 7.632 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000×1000 Matrix{Float64}:\n",
       " 1.0  0.0326361  -0.0181891   0.0166341   …   0.00767459    0.0104462\n",
       " 0.0  1.0        -0.00041394  0.00540477     -0.10473      -0.00184638\n",
       " 0.0  0.0         1.0         0.0177243      -0.000167982  -0.00929269\n",
       " 0.0  0.0         0.0         1.0            -2.1684e-19   -0.00329412\n",
       " 0.0  0.0         0.0         0.0             1.2618e-5    -0.000859109\n",
       " 0.0  0.0         0.0         0.0         …   2.98156e-19   0.0507852\n",
       " 0.0  0.0         0.0         0.0             2.88077e-5   -0.0117684\n",
       " 0.0  0.0         0.0         0.0            -0.000116338  -0.0104104\n",
       " 0.0  0.0         0.0         0.0             2.67669e-5   -0.00728978\n",
       " 0.0  0.0         0.0         0.0             2.331e-5     -0.00253932\n",
       " 0.0  0.0         0.0         0.0         …   0.00150567   -0.00353229\n",
       " 0.0  0.0         0.0         0.0             0.00509611    0.0109818\n",
       " 0.0  0.0         0.0         0.0            -0.0126056     0.0126193\n",
       " ⋮                                        ⋱                \n",
       " 0.0  0.0         0.0         0.0            -0.0182409    -0.0106091\n",
       " 0.0  0.0         0.0         0.0            -0.00116818    0.00235988\n",
       " 0.0  0.0         0.0         0.0         …  -0.00204165   -0.0130619\n",
       " 0.0  0.0         0.0         0.0             0.00840741   -0.00771116\n",
       " 0.0  0.0         0.0         0.0             0.983214      0.00734632\n",
       " 0.0  0.0         0.0         0.0             0.0156904    -0.00610452\n",
       " 0.0  0.0         0.0         0.0             0.00396482   -0.0110234\n",
       " 0.0  0.0         0.0         0.0         …  -0.00290343   -0.0109804\n",
       " 0.0  0.0         0.0         0.0             0.00723443   -0.013669\n",
       " 0.0  0.0         0.0         0.0             0.99998       0.00738016\n",
       " 0.0  0.0         0.0         0.0             1.0           0.00737766\n",
       " 0.0  0.0         0.0         0.0             0.0           1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time bm[1:1000, 1:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22.868453 seconds (61.51 k allocations: 3.108 MiB, 0.31% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×50 Matrix{Float64}:\n",
       " 1.0  -0.0181891   0.00802797  -0.0421512  …  0.0143534     0.0172358\n",
       " 0.0  -0.00041394  0.00232164  -0.0112066     5.39059e-5   -0.0188801\n",
       " 0.0   1.0         0.0         -0.0282011     0.000733417  -0.0277042"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing non-contiguous region is slow\n",
    "@time bm[1:3, 1:2:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.827913 seconds (33.87 k allocations: 1.972 MiB, 1.93% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100×100 Matrix{Float64}:\n",
       " 1.0  0.0326361  -0.0181891   0.0166341   …   0.0172358    -0.00857776\n",
       " 0.0  1.0        -0.00041394  0.00540477     -0.0188801     0.05925\n",
       " 0.0  0.0         1.0         0.0177243      -0.0277042    -0.000268512\n",
       " 0.0  0.0         0.0         1.0            -0.000468342  -0.000966708\n",
       " 0.0  0.0         0.0         0.0            -7.47553e-5   -0.00298802\n",
       " 0.0  0.0         0.0         0.0         …   0.00207735    0.00152017\n",
       " 0.0  0.0         0.0         0.0             0.00256425    0.00136302\n",
       " 0.0  0.0         0.0         0.0             0.0137416    -0.000680087\n",
       " 0.0  0.0         0.0         0.0            -0.020671      0.000342287\n",
       " 0.0  0.0         0.0         0.0             0.00195264   -0.00451867\n",
       " 0.0  0.0         0.0         0.0         …   0.0018285    -0.00406988\n",
       " 0.0  0.0         0.0         0.0             0.0152337     0.0100539\n",
       " 0.0  0.0         0.0         0.0             0.0193       -0.0136828\n",
       " ⋮                                        ⋱                \n",
       " 0.0  0.0         0.0         0.0            -0.0164688    -0.014087\n",
       " 0.0  0.0         0.0         0.0             0.00237332   -0.0076453\n",
       " 0.0  0.0         0.0         0.0         …   0.0307489     0.00414244\n",
       " 0.0  0.0         0.0         0.0            -0.0163064    -0.006019\n",
       " 0.0  0.0         0.0         0.0            -0.0133323    -0.0457871\n",
       " 0.0  0.0         0.0         0.0            -0.00121495   -0.0130313\n",
       " 0.0  0.0         0.0         0.0            -0.0107884    -0.0132339\n",
       " 0.0  0.0         0.0         0.0         …   0.0466568     0.00123582\n",
       " 0.0  0.0         0.0         0.0             0.065021      0.00380187\n",
       " 0.0  0.0         0.0         0.0             0.086153     -0.0106022\n",
       " 0.0  0.0         0.0         0.0             1.0          -0.00980493\n",
       " 0.0  0.0         0.0         0.0             0.0           1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "py\"\"\"\n",
    "from hail.linalg import BlockMatrix\n",
    "import numpy as np\n",
    "def convert(bm, range_start, range_end):\n",
    "    return bm[range_start:range_end, range_start:range_end].to_numpy()\n",
    "\"\"\"\n",
    "@time py\"convert\"(bm.bm, 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hail.linalg import BlockMatrix\n",
    "# import numpy as np\n",
    "bm_files = \"UKBB.EUR.ldadj.bm\"\n",
    "bm = BlockMatrix.read(bm_files)\n",
    "\n",
    "# get a slice fast\n",
    "x = bm[0:10000, 0:10000]\n",
    "X = x.to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(bm, range1, range2)\n",
    "    return bm[range1, range2].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
